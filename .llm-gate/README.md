# LiteLLM Proxy Project

This project was generated by llm-anygate-cli to set up a LiteLLM proxy server.

## Configuration

- **Port**: 4567
- **Master Key**: sk-dummy (change for production!)
- **Config File**: config.yaml

## Setup

1. Install LiteLLM with proxy support:
   ```bash
   pip install 'litellm[proxy]'
   ```

2. Set up environment variables:
   - Copy `env.example` to `.env`
   - Add your API keys to `.env`

3. Start the proxy server:
   ```bash
   llm-anygate-cli start
   ```

## Usage

The proxy server provides an OpenAI-compatible API at:
```
http://localhost:4567
```

### Example with curl:
```bash
curl http://localhost:4567/v1/models \
  -H "Authorization: Bearer sk-dummy"
```

### Example with OpenAI Python client:
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:4567/v1",
    api_key="sk-dummy"
)

response = client.chat.completions.create(
    model="gpt-5-2025-08-07",  # or any model from config.yaml
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## Models

Check `config.yaml` for available models and their configurations.

## Security

**Important**: The default master key `sk-dummy` is not secure. 
For production use, generate a secure key and update:
1. The `master_key` in `config.yaml`
2. The `LITELLM_MASTER_KEY` in your `.env` file

## Troubleshooting

- **Port already in use**: Use `--port` option with `llm-anygate-cli start` or update anygate.yaml
- **Authentication errors**: Check your API keys in .env file
- **Model not found**: Verify model names in config.yaml

## Documentation

- [LiteLLM Documentation](https://docs.litellm.ai/)
- [LiteLLM Proxy Guide](https://docs.litellm.ai/docs/simple_proxy)
