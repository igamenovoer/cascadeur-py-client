<execution>
  <constraint>
    ## 技术架构约束
    - **Python版本兼容**：支持Python 3.8+，兼容异步编程特性
    - **依赖管理限制**：优先使用轻量级库，避免重依赖冲突
    - **内存使用控制**：大规模爬取时内存使用不超过可用资源70%
    - **网络环境适应**：支持代理、VPN、不稳定网络环境
    - **合规性硬约束**：严格遵循robots.txt、请求频率限制、版权规范
    
    ## LLM集成约束
    - **Token预算管理**：单次请求Token数量控制，成本预算限制
    - **API调用限制**：遵循LLM服务商的频率限制和并发约束
    - **响应时间要求**：LLM处理延迟对整体爬取效率的影响控制
    - **数据隐私约束**：敏感数据不发送给第三方LLM服务
  </constraint>

  <rule>
    ## 爬虫开发强制规则
    - **robots.txt优先**：每次爬取前必须检查并遵守robots.txt规则
    - **请求间隔强制**：同一域名请求间隔不少于1秒，复杂站点延长至3-5秒
    - **User-Agent真实性**：必须使用真实浏览器User-Agent，定期更新版本
    - **错误处理完整性**：每个网络请求都必须有超时设置和异常捕获
    - **日志记录规范**：关键操作、错误信息、性能指标必须详细记录
    
    ## 数据处理强制规则
    - **编码处理统一**：统一使用UTF-8编码处理所有文本数据
    - **数据验证必需**：爬取数据必须经过格式验证和完整性检查
    - **去重机制强制**：URL去重、内容去重、结构化数据去重
    - **存储事务性**：数据存储操作必须支持事务回滚和一致性保证
    
    ## LLM集成强制规则
    - **输入清洗必需**：发送给LLM的内容必须经过HTML标签清理和格式化
    - **输出验证强制**：LLM返回结果必须经过格式验证和合理性检查
    - **失败降级机制**：LLM处理失败时必须有传统方法作为后备方案
    - **批处理优化**：优先使用批量请求减少API调用次数和成本
  </rule>

  <guideline>
    ## 架构设计指导
    - **模块化优先**：爬虫引擎、数据处理、LLM集成、存储各模块独立
    - **配置外置化**：所有可变参数通过配置文件管理，避免硬编码
    - **可扩展性考虑**：设计时考虑水平扩展和分布式部署可能性
    - **监控友好性**：提供丰富的metrics接口便于监控和调试
    
    ## 反爬策略指导
    - **渐进式升级**：从最简单的方法开始，遇到阻碍时逐步升级技术
    - **多样性策略**：IP池、User-Agent池、请求模式随机化
    - **智能检测**：根据响应状态自动调整爬取策略和频率
    - **优雅降级**：遇到严重反爬时能够优雅停止并保存进度
    
    ## 内容处理指导
    - **结构化优先**：优先提取结构化数据（JSON-LD、microdata等）
    - **语义理解**：利用HTML语义标签和LLM语义理解提高准确性
    - **质量控制**：建立内容质量评估标准和自动化检查机制
    - **版本管理**：对提取规则和处理逻辑进行版本管理和A/B测试
  </guideline>

  <process>
    ## 完整爬虫开发流程
    
    ### Phase 1: 需求分析与方案设计 (1-2天)
    
    ```mermaid
    flowchart TD
        A[需求收集] --> B[目标网站分析]
        B --> C[数据结构设计]
        C --> D[技术选型决策]
        D --> E[架构设计文档]
        E --> F[开发计划制定]
    ```
    
    **关键输出**：
    - 需求文档和数据字典
    - 网站分析报告（结构、反爬、API接口）
    - 技术方案设计（工具选择、架构图、数据流）
    - 项目计划和里程碑
    
    ### Phase 2: 基础框架开发 (3-5天)
    
    ```mermaid
    graph LR
        A[请求封装模块] --> B[HTML解析模块]
        B --> C[数据提取模块] 
        C --> D[存储模块]
        D --> E[配置管理模块]
        E --> F[日志监控模块]
    ```
    
    **开发重点**：
    - 建立可重用的请求处理类
    - 实现灵活的内容解析框架
    - 设计可扩展的数据存储接口
    - 完善的异常处理和日志记录
    
    ### Phase 3: 反爬应对机制 (2-4天)
    
    ```mermaid
    flowchart TD
        A[基础爬虫测试] --> B{遇到反爬?}
        B -->|是| C[分析反爬类型]
        B -->|否| I[进入下一阶段]
        C --> D{反爬类型}
        D -->|请求限制| E[添加延迟和代理]
        D -->|JS渲染| F[集成Selenium/Playwright]
        D -->|验证码| G[OCR或打码平台]
        D -->|行为检测| H[模拟人类操作]
        E --> B
        F --> B
        G --> B
        H --> B
    ```
    
    **应对策略**：
    - IP代理池管理和轮换
    - 请求头和会话管理
    - JavaScript渲染引擎集成
    - 验证码识别和处理
    
    ### Phase 4: LLM集成开发 (2-3天)
    
    ```mermaid
    graph TD
        A[原始HTML内容] --> B[内容预处理]
        B --> C[LLM API调用]
        C --> D[结果验证处理]
        D --> E[结构化数据输出]
        
        B --> B1[HTML清洗]
        B --> B2[文本格式化] 
        B --> B3[内容分块]
        
        C --> C1[批量请求优化]
        C --> C2[异步调用管理]
        C --> C3[错误重试机制]
        
        D --> D1[格式验证]
        D --> D2[内容合理性检查]
        D --> D3[置信度评估]
    ```
    
    **集成要点**：
    - 设计高效的内容预处理流水线
    - 实现智能的LLM调用策略（批处理、缓存）
    - 建立完善的结果验证和质量控制机制
    - 优化Token使用和API调用成本
    
    ### Phase 5: 测试与优化 (2-3天)
    
    ```mermaid
    flowchart LR
        A[单元测试] --> B[集成测试]
        B --> C[性能测试]
        C --> D[稳定性测试]
        D --> E[优化调整]
        E --> F[部署准备]
    ```
    
    **测试重点**：
    - 各模块功能正确性验证
    - 端到端流程完整性测试
    - 高并发和长时间运行测试
    - 异常场景和恢复能力测试
    
    ### Phase 6: 部署与监控 (1-2天)
    
    ```mermaid
    graph TD
        A[环境准备] --> B[应用部署]
        B --> C[监控配置]
        C --> D[告警设置]
        D --> E[运行验证]
        E --> F[文档整理]
    ```
    
    **部署要素**：
    - 容器化部署和服务编排
    - 实时监控和告警机制
    - 日志聚合和分析
    - 备份恢复和灾难预案
  </process>

  <criteria>
    ## 项目成功标准
    
    ### 功能完整性标准
    - ✅ 能够成功爬取目标网站90%以上的目标数据
    - ✅ LLM处理准确率达到85%以上
    - ✅ 数据结构化程度满足下游应用需求
    - ✅ 支持断点续传和增量更新
    
    ### 性能指标标准
    - ✅ 单机爬取速度：100+ pages/hour (简单站点)，20+ pages/hour (复杂站点)
    - ✅ LLM处理延迟：平均响应时间 < 5秒/页面
    - ✅ 系统稳定性：连续运行24小时成功率 > 95%
    - ✅ 资源使用率：CPU < 70%，内存 < 70%，网络带宽合理
    
    ### 质量保证标准
    - ✅ 数据准确性：结构化数据准确率 > 90%
    - ✅ 完整性检查：关键字段缺失率 < 5%
    - ✅ 去重效率：重复数据比例 < 2%
    - ✅ 错误处理：异常情况自动恢复率 > 80%
    
    ### 合规性标准
    - ✅ 严格遵守robots.txt和网站使用条款
    - ✅ 请求频率控制在合理范围内
    - ✅ 数据使用符合法律法规要求
    - ✅ 隐私数据处理符合数据保护规范
    
    ### 可维护性标准
    - ✅ 代码覆盖率 > 80%
    - ✅ 文档完整度 > 90%
    - ✅ 配置灵活性支持快速调整
    - ✅ 监控告警机制完善及时
  </criteria>
</execution>